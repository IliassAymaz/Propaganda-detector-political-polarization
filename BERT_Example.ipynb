{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:  1.0.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_transformers import BertTokenizer, BertForTokenClassification, BertForSequenceClassification\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "print(\"PyTorch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in total:  30522\n"
     ]
    }
   ],
   "source": [
    "vocab = tokenizer.vocab\n",
    "print(\"Number of tokens in total: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token               index          \n",
      "-------------------------\n",
      "tighten             21245\n",
      "physician            7522\n",
      "ursula              20449\n",
      "premise             18458\n",
      "313                 22997\n",
      "lacy                19959\n",
      "bassist              9858\n",
      "widows              24835\n",
      "eireann             29157\n",
      "corner               3420\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_tokens = random.sample(list(vocab), 10)\n",
    "random_ids = [vocab[t] for t in random_tokens]\n",
    "\n",
    "print(\"{0:20}{1:15}\".format(\"token\", \"index\"))\n",
    "print(\"-\" * 25)\n",
    "for t, id in zip(random_tokens, random_ids):\n",
    "    print(\"{0:15}{1:10}\".format(t, id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] The larger-than-usual outbreak had helped spread the bacteria that causes the plague more widely.\n",
      "['[CLS]', 'the', 'larger', '-', 'than', '-', 'usual', 'outbreak', 'had', 'helped', 'spread', 'the', 'bacteria', 'that', 'causes', 'the', 'plague', 'more', 'widely', '.']\n",
      "[101, 1996, 3469, 1011, 2084, 1011, 5156, 8293, 2018, 3271, 3659, 1996, 10327, 2008, 5320, 1996, 11629, 2062, 4235, 1012]\n"
     ]
    }
   ],
   "source": [
    "text = \"[CLS] The larger-than-usual outbreak had helped spread the bacteria that causes the plague more widely.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(text)\n",
    "print(tokens)\n",
    "print(tokens_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4211, grad_fn=<NllLossBackward>), tensor([[[-0.2600,  0.8161],\n",
       "          [ 0.1547,  0.4057],\n",
       "          [ 0.0310,  0.6644],\n",
       "          [-0.4678,  0.2201],\n",
       "          [-0.5757,  0.8247],\n",
       "          [-0.4877,  0.2871],\n",
       "          [ 0.0616,  0.7254],\n",
       "          [ 0.1117,  0.7215],\n",
       "          [-0.0703,  0.3294],\n",
       "          [ 0.0458,  0.4760],\n",
       "          [-0.0522,  0.4958],\n",
       "          [-0.2616,  0.3395],\n",
       "          [-0.0715,  0.6997],\n",
       "          [-0.2226,  0.5437],\n",
       "          [-0.2867,  0.6649],\n",
       "          [-0.3434,  0.4562],\n",
       "          [-0.2732,  0.9214],\n",
       "          [-0.2590,  0.6428],\n",
       "          [-0.3579,  0.6119],\n",
       "          [-0.0466,  0.5765]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "tokens_tensor = torch.tensor([tokens_ids])\n",
    "labels_ids = [1] * len(tokens)\n",
    "labels_ids[10] = 0  #spread\n",
    "labels_tensor = torch.tensor([labels_ids])\n",
    "model.config.output_hidden_states = True\n",
    "outputs = model(tokens_tensor, labels=labels_tensor)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_labels\": 2,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"pruned_heads\": {},\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "model.config.output_hidden_states = True\n",
    "outputs = model(input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7053, grad_fn=<NllLossBackward>),\n",
       " tensor([[0.0467, 0.0225]], grad_fn=<AddmmBackward>))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-90ad01690a8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
